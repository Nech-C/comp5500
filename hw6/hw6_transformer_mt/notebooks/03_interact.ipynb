{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nech\\anaconda3\\envs\\comp5500-hw6\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "from transformer_mt.modeling_transformer import TransfomerEncoderDecoderModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransfomerEncoderDecoderModel(\n",
       "  (positional_emb): Embedding(128, 512)\n",
       "  (encoder_embeddings): Embedding(32000, 512)\n",
       "  (decoder_embeddings): Embedding(32000, 512)\n",
       "  (out_proj): Linear(in_features=512, out_features=32000, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (encoder): ModuleList(\n",
       "    (0-5): 6 x TransformerEncoderLayer(\n",
       "      (self_attention): MultiHeadAttention(\n",
       "        (k): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (q): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (v): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (mix): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (att_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (fcn): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      )\n",
       "      (fcn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (decoder): ModuleList(\n",
       "    (0-5): 6 x TransformerDecoderLayer(\n",
       "      (self_attention): MultiHeadAttention(\n",
       "        (k): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (q): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (v): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (mix): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (corss_attention): MultiHeadAttention(\n",
       "        (k): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (q): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (v): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (mix): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (att_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (cross_att_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (fcn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (fcn): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You probably need to modify the paths to your tokenizers\n",
    "# For tokenizers, you need to provide the path to a directory that contains the tokenizer.json file\n",
    "# For model, you need to provide the path to a directory that contains the model.pt file\n",
    "\n",
    "source_tokenizer = PreTrainedTokenizerFast.from_pretrained(\"../en_de_output_dir/en_tokenizer\")\n",
    "target_tokenizer = PreTrainedTokenizerFast.from_pretrained(\"../en_de_output_dir/de_tokenizer\")\n",
    "\n",
    "model = TransfomerEncoderDecoderModel.from_pretrained(\"../en_de_output_dir\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task\n",
    "\n",
    "Try out your model. Feel free to use your own sentences and to compare the model outputs to Google Translate.\n",
    "Find at least three sentences that are translated well, and one that is translated badly.\n",
    "\n",
    "Feel free to change beam size and max_length parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Gro√üe Sprach modelle.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = source_tokenizer.encode(\"large language models.\", return_tensors=\"pt\")\n",
    "output_ids = model.generate(\n",
    "    input_ids,\n",
    "    max_length=10,\n",
    "    bos_token_id=target_tokenizer.bos_token_id,\n",
    "    eos_token_id=target_tokenizer.eos_token_id,\n",
    "    pad_token_id=target_tokenizer.pad_token_id,\n",
    ")\n",
    "target_tokenizer.decode(output_ids[0])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f86d5ac646110a331c60478f9400a1a64d0cd523be90d887457228f9723636b5"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('nlp_class')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
